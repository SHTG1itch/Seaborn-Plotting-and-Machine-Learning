{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra Practice for Machine Learning\n",
    "For this you will be working with the `cars` dataset from `vega_datasets`. You may have seen them before, but if not, they are described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cars = pd.read_csv('cars.csv').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cars` dataset is a dataset with a bunch of different models of car, with several different statistics about each of them, including their horsepower, acceleration, etc., the year they were released, and their country of origin. Here's what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Miles_per_Gallon</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight_in_lbs</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Year</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buick skylark 320</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plymouth satellite</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amc rebel sst</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ford torino</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n",
       "0  chevrolet chevelle malibu              18.0          8         307.0   \n",
       "1          buick skylark 320              15.0          8         350.0   \n",
       "2         plymouth satellite              18.0          8         318.0   \n",
       "3              amc rebel sst              16.0          8         304.0   \n",
       "4                ford torino              17.0          8         302.0   \n",
       "\n",
       "   Horsepower  Weight_in_lbs  Acceleration        Year Origin  \n",
       "0       130.0           3504          12.0  1970-01-01    USA  \n",
       "1       165.0           3693          11.5  1970-01-01    USA  \n",
       "2       150.0           3436          11.0  1970-01-01    USA  \n",
       "3       150.0           3433          12.0  1970-01-01    USA  \n",
       "4       140.0           3449          10.5  1970-01-01    USA  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML with Quantitative Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train a model that, given the `cars` dataset, will predict the Horsepower of a car. Think about the type of data you are trying to predict - what model (of the ones we have already seen) should you use to predict quantitative data? Make sure to split training and testing data, and check the mean squared error of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171.95384615384614\n",
      "53.63846153846154\n",
      "66.13609467455622\n",
      "56.63609467455621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = cars[[\"Weight_in_lbs\",\"Acceleration\",\"Displacement\"]]\n",
    "y = cars[[\"Horsepower\"]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.33, random_state=86)\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(x, y, test_size=0.86, random_state=42)\n",
    "x_train4, x_test4, y_train4, y_test4 = train_test_split(x, y, test_size=0.86, random_state=86)\n",
    "reg = DecisionTreeRegressor(random_state = 1).fit(x_train,y_train)\n",
    "pred = reg.predict(x_test)\n",
    "mse1 = mean_squared_error(pred,y_test)\n",
    "mse2 = mean_squared_error(reg.predict(x_test2),y_test2)\n",
    "mse3 = mean_squared_error(reg.predict(x_test3),y_test3)\n",
    "mse4 = mean_squared_error(reg.predict(x_test4),y_test4)\n",
    "print(mse1)\n",
    "print(mse2)\n",
    "print(mse3)\n",
    "print(mse4)\n",
    "# Enter the rest of your solution here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: How does accuracy or mean squared error change with the split between training and testing data? Do at least three different splits with this data to see.\n",
    "As the test size increases, the accuracy seems to increase, and as the shuffling applied to the data increases the accuracy also seems to increase. This is because of bias and variance, with the model becoming more accurate by being given more random values(it can't make absurd connections between data points) and it becomes more accurate given a larger testing set(another way that it can't make absurb connections, and also provides more of an evening out of mistakes due to statistics). However, if you were to increase both test size and randomness, the accuracy starts decreasing a little, this is because the benefits of both can interfere with each other leading to more mistakes. Also, the reason that I think there is a high mean squared error for the first test set is because the test sets that follow could use the same data that is present in earlier training sets leading to the model having the answers, I verified this by commenting out the original testing set and replacing it with one of the other ones, and saw that the mean squared error was super high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Bonus: Testing hyperparameters. What maximum depth has the greatest accuracy in our testing set. If we want to do this without making decision off our our training dataset, we need to split our data into three categories: train, test, and development. Then we can compare our changes in how it affects the development dataset and not the training dataset so we can do a final evaluation at the end with our final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set MSE (depth, mse):\n",
      "1 370.6203\n",
      "2 315.7442\n",
      "3 333.1648\n",
      "4 340.2377\n",
      "5 278.9287\n",
      "6 261.0662\n",
      "7 319.477\n",
      "8 256.1361\n",
      "9 236.3006\n",
      "10 316.8642\n",
      "11 241.314\n",
      "12 254.8966\n",
      "13 304.069\n",
      "14 304.069\n",
      "15 304.069\n",
      "16 304.069\n",
      "17 304.069\n",
      "18 304.069\n",
      "19 304.069\n",
      "20 304.069\n",
      "None 304.069\n",
      "\n",
      "Best depth on dev set: 9 with MSE 236.3006\n",
      "Test set MSE with best depth: 191.3629\n",
      "Default (no max_depth) test MSE: 171.9538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = cars[[\"Weight_in_lbs\",\"Acceleration\",\"Displacement\"]]\n",
    "y = cars[[\"Horsepower\"]]\n",
    "# File: ML-Extra-Practice.ipynb — replaced the \"More Bonus\" regressor code cell with a complete hyperparameter tuning snippet that:\n",
    "# Splits data into train / dev / test (train -> train_final + dev),\n",
    "# Tries max_depth values 1..20 and None,\n",
    "# Measures MSE on the dev set and picks the depth with lowest dev MSE,\n",
    "# Retrains the final regressor on the full training set (train + dev) using the chosen depth,\n",
    "# Evaluates and prints test MSE, and compares to the default (no max_depth).\n",
    "# Why this works (summary)\n",
    "\n",
    "# Hold-out dev set prevents choosing hyperparameters that overfit the test set — we tune on dev only.\n",
    "# Searching over max_depth controls bias vs. variance:\n",
    "# Small depth → simpler model (higher bias, lower variance),\n",
    "# Large depth / None → complex model (lower bias, higher variance).\n",
    "# Choosing depth by dev MSE finds the best trade-off for this dataset.\n",
    "# Retraining on the full training set (train + dev combined) before final evaluation gives the final model more data to learn from while still preserving an untouched test set for an honest estimate of generalization.\n",
    "# Split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "# Hold out a development set from the training data for hyperparameter tuning\n",
    "x_train_final, x_dev, y_train_final, y_dev = train_test_split(x_train, y_train, test_size=0.33, random_state=85)\n",
    "\n",
    "# Grid-search over possible `max_depth` values using the development set (lower MSE is better)\n",
    "depths = list(range(1, 21)) + [None]\n",
    "best_depth = None\n",
    "best_mse = float('inf')\n",
    "results = []\n",
    "for d in depths:\n",
    "    reg = DecisionTreeRegressor(max_depth=d, random_state=1)\n",
    "    # fit on the training portion (not the dev set)\n",
    "    reg.fit(x_train_final, y_train_final.values.ravel())\n",
    "    y_dev_pred = reg.predict(x_dev)\n",
    "    mse = mean_squared_error(y_dev, y_dev_pred)\n",
    "    results.append((d, mse))\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_depth = d\n",
    "\n",
    "# Show development MSEs for each depth\n",
    "print('Dev set MSE (depth, mse):')\n",
    "for d, mse in results:\n",
    "    print(d, round(mse, 4))\n",
    "\n",
    "print(f\"\\nBest depth on dev set: {best_depth} with MSE {best_mse:.4f}\")\n",
    "\n",
    "# Retrain on the full training data (train + dev = x_train) with the chosen hyperparameter\n",
    "final_reg = DecisionTreeRegressor(max_depth=best_depth, random_state=1)\n",
    "final_reg.fit(x_train, y_train.values.ravel())\n",
    "y_test_pred = final_reg.predict(x_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test set MSE with best depth: {test_mse:.4f}\")\n",
    "\n",
    "# Compare to default (no max_depth) to see whether limiting depth helped\n",
    "default_reg = DecisionTreeRegressor(random_state=1)\n",
    "default_reg.fit(x_train, y_train.values.ravel())\n",
    "default_mse = mean_squared_error(y_test, default_reg.predict(x_test))\n",
    "print(f\"Default (no max_depth) test MSE: {default_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML with Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create, train, and test a model that will predict the country of origin for the `cars` dataset. Remember, this is categorical data, so you will need to use a different type of model (of the ones we have already seen) than you did for the `Horsepower` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7538461538461538\n",
      "0.9384615384615385\n",
      "0.9023668639053254\n",
      "0.9201183431952663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = cars[[\"Cylinders\",\"Miles_per_Gallon\",\"Displacement\"]]\n",
    "y = cars[[\"Origin\"]]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.33, random_state=86)\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(x, y, test_size=0.86, random_state=42)\n",
    "x_train4, x_test4, y_train4, y_test4 = train_test_split(x, y, test_size=0.86, random_state=86)\n",
    "reg = DecisionTreeClassifier(random_state = 1).fit(x_train,y_train)\n",
    "pred = reg.predict(x_test)\n",
    "ac1 = accuracy_score(pred,y_test)\n",
    "ac2 = accuracy_score(reg.predict(x_test2),y_test2)\n",
    "ac3 = accuracy_score(reg.predict(x_test3),y_test3)\n",
    "ac4 = accuracy_score(reg.predict(x_test4),y_test4)\n",
    "print(ac1)\n",
    "print(ac2)\n",
    "print(ac3)\n",
    "print(ac4)\n",
    "# Enter the rest of your solution here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: How does accuracy or mean squared error change with the split between training and testing data? Do at least three different splits with this data to see. Unlike for the regression, in this model, adjusting the randomness or shuffling is what produced the most accurate predictions. As shown in the 3 splits, we can see that increasing the test size reduces the accuracy, as the model doesn't have enough data to work with, and increasing the randomness prevents bias which improves the accuracy as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1288144275.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfor parameter in model.get_params():\u001b[39m\n                                        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Bonus: Testing hyperparameters. What maximum depth (or other hyperparameter) has the greatest accuracy in our testing set? If we want to do this without making decision off our our training dataset, we need to split our data into three categories: train, test, and development. Then we can compare our changes in how it affects the development dataset and not the training dataset so we can do a final evaluation at the end with our final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set accuracies (depth, accuracy):\n",
      "1 0.6897\n",
      "2 0.7471\n",
      "3 0.7586\n",
      "4 0.6667\n",
      "5 0.7586\n",
      "6 0.7931\n",
      "7 0.7931\n",
      "8 0.7931\n",
      "9 0.8046\n",
      "10 0.7931\n",
      "11 0.8161\n",
      "12 0.8161\n",
      "13 0.8161\n",
      "14 0.8161\n",
      "15 0.8161\n",
      "16 0.8161\n",
      "17 0.8161\n",
      "18 0.8161\n",
      "19 0.8161\n",
      "20 0.8161\n",
      "None 0.8161\n",
      "\n",
      "Best depth on dev set: 11 with accuracy 0.8161\n",
      "Test set accuracy with best depth: 0.7923\n",
      "Default (no max_depth) test accuracy: 0.7923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = cars[[\"Weight_in_lbs\",\"Miles_per_Gallon\",\"Displacement\"]]\n",
    "y = cars[[\"Origin\"]]\n",
    "\n",
    "# File: ML-Extra-Practice.ipynb\n",
    "# Change: Replaced the final classifier code cell with a hyperparameter search loop that:\n",
    "# Splits training data into training + development (dev) sets,\n",
    "# Trains DecisionTreeClassifier models across a range of max_depth values,\n",
    "# Selects the best max_depth based on dev accuracy,\n",
    "# Retrains on the full training set with that depth, and\n",
    "# Evaluates accuracy on the held-out test set (and prints a comparison to the default tree).\n",
    "# Why this works (concise)\n",
    "\n",
    "# Hold-out dev set: Using a separate development set prevents tuning decisions from overfitting to the test set; hyperparameters are chosen using dev performance only.\n",
    "# Grid search over depth: Trying multiple max_depth values finds the depth that best balances bias vs. variance for this dataset (small depth → high bias, large depth → high variance).\n",
    "# Retrain on full train before final test: After selecting the hyperparameter on dev, retraining on the full training data (train + dev combined — here we use the original x_train which contains both parts) gives the model access to more data for a better final estimate on the test set.\n",
    "# Compare to default: Printing default (no max_depth) test accuracy shows whether limiting depth improved generalization.\n",
    "\n",
    "# Split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "# Hold out a development set from the training data for hyperparameter tuning\n",
    "x_train_final, x_dev, y_train_final, y_dev = train_test_split(x_train, y_train, test_size=0.33, random_state=85)\n",
    "\n",
    "# Grid-search over possible `max_depth` values using the development set\n",
    "depths = list(range(1, 21)) + [None]\n",
    "best_depth = None\n",
    "best_acc = -1.0\n",
    "results = []\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=1)\n",
    "    # fit on the training portion (not the dev set)\n",
    "    clf.fit(x_train_final, y_train_final.values.ravel())\n",
    "    y_dev_pred = clf.predict(x_dev)\n",
    "    acc = accuracy_score(y_dev, y_dev_pred)\n",
    "    results.append((d, acc))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_depth = d\n",
    "\n",
    "# Show development accuracies for each depth\n",
    "print('Dev set accuracies (depth, accuracy):')\n",
    "for d, acc in results:\n",
    "    print(d, round(acc, 4))\n",
    "\n",
    "print(f'\\nBest depth on dev set: {best_depth} with accuracy {best_acc:.4f}')\n",
    "\n",
    "# Retrain on the full training data (train + dev = x_train) with the chosen hyperparameter\n",
    "final_clf = DecisionTreeClassifier(max_depth=best_depth, random_state=1)\n",
    "final_clf.fit(x_train, y_train.values.ravel())\n",
    "y_test_pred = final_clf.predict(x_test)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Test set accuracy with best depth: {test_acc:.4f}')\n",
    "\n",
    "# Compare to default (no max_depth) to see whether limiting depth helped\n",
    "default_clf = DecisionTreeClassifier(random_state=1)\n",
    "default_clf.fit(x_train, y_train.values.ravel())\n",
    "default_acc = accuracy_score(y_test, default_clf.predict(x_test))\n",
    "print(f'Default (no max_depth) test accuracy: {default_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
